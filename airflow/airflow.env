# Airflow Executor Configuration
AIRFLOW__CORE__EXECUTOR=CeleryExecutor

# Celery Configuration
AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/0
AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://postgres:postgres@postgres:5432/airflow

# Default Database Connection
AIRFLOW_CONN_POSTGRES_DEFAULT=postgresql+psycopg2://airflow:airflow@postgres/postgres
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://postgres:postgres@postgres:5432/airflow

# Security and Logging
AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
AIRFLOW__CORE__LOGGING_LEVEL=INFO
API_KEY_CONFIG=/opt/airflow/config/secrets.conf
GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/.google/nasa-neows-c12d19292fad.json

# Example DAGs
AIRFLOW__CORE__LOAD_EXAMPLES=False

# API Authentication
AIRFLOW__API__AUTH_BACKEND=airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session


# Google Cloud Configuration
AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=google-cloud-platform://
GCP_PROJECT_ID=nasa-neows
GCP_GCS_BUCKET=de_data_lake_nasa-neows


# BigQuery Configuration
BIGQUERY_STAGING_DATASET= "${TF_VAR_BQ_DATASET_STAGING:-de_dataset_staging}"
BIGQUERY_WAREHOUSE_DATASET= "${TF_VAR_BQ_DATASET_WAREHOUSE:-de_dataset_warehouse}"


AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=60

